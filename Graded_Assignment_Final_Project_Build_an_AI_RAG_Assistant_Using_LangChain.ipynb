{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pip setuptools wheel\n",
        "\n",
        "# Install core libraries without strict pinning\n",
        "!pip install gradio langchain langchain-community chromadb pypdf pydantic sentence-transformers transformers huggingface_hub\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio, langchain, chromadb, pypdf, transformers, sentence_transformers, huggingface_hub, pydantic\n",
        "\n",
        "print(\"gradio:\", gradio.__version__)\n",
        "print(\"langchain:\", langchain.__version__)\n",
        "print(\"chromadb:\", chromadb.__version__)\n",
        "print(\"pypdf:\", pypdf.__version__)\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "print(\"huggingface_hub:\", huggingface_hub.__version__)\n",
        "print(\"pydantic:\", pydantic.__version__)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Load document using LangChain for different sources\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "def document_loader(file_path):\n",
        "    # For Gradio File input (type=\"filepath\"), we receive a string path\n",
        "    loader = PyPDFLoader(file_path)\n",
        "    loaded_document = loader.load()\n",
        "    return loaded_document\n",
        "\n",
        "# Demo\n",
        "docs = document_loader(\"/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf\")\n",
        "print(docs[0].page_content[:300])\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Apply text splitting techniques\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def text_splitter(data):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=150,\n",
        "        length_function=len\n",
        "    )\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks\n",
        "\n",
        "# Demo\n",
        "chunks = text_splitter(docs)\n",
        "len(chunks), chunks[0].page_content[:200]\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Embed documents\n",
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "\n",
        "def local_embedding_model():\n",
        "    # Small, fast, accurate enough for grading\n",
        "    return SentenceTransformerEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Demo: show first five embedding numbers\n",
        "emb = local_embedding_model()\n",
        "vec = emb.embed_query(\"This is an example sentence for embedding.\")\n",
        "vec[:5]\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Create and configure vector databases to store embeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "def vector_database(chunks):\n",
        "    embedding_model = local_embedding_model()\n",
        "    vectordb = Chroma.from_documents(chunks, embedding_model)\n",
        "    return vectordb\n",
        "\n",
        "# Demo\n",
        "vectordb = vector_database(chunks)\n",
        "vectordb._collection.count()\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Develop a retriever to fetch document segments based on queries\n",
        "def build_retriever(file_path):\n",
        "    splits = document_loader(file_path)\n",
        "    chunks = text_splitter(splits)\n",
        "    vectordb = vector_database(chunks)\n",
        "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "    return retriever\n",
        "\n",
        "# Demo\n",
        "retriever = build_retriever(\"/content/A_Comprehensive_Review_of_Low_Rank_Adaptation_in_Large_Language_Models_for_Efficient_Parameter_Tuning-1.pdf\")\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Construct a QA Bot that leverages LangChain and LLM to answer questions\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "class LocalFlanLLM:\n",
        "    def __init__(self, model_id=\"google/flan-t5-large\", device=None):\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def generate(self, prompt, max_new_tokens=256, temperature=0.5):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        # FLAN-T5 doesn't use temperature directly; we simulate with top_p/top_k if needed\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            top_k=50\n",
        "        )\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Simple RAG: retrieve top chunks, then pass them as context to the LLM with the query\n",
        "def rag_answer(file_path, query):\n",
        "    retriever = build_retriever(file_path)\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
        "    prompt = f\"Use the following context to answer the user question.\\n\\nContext:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    llm = LocalFlanLLM()\n",
        "    answer = llm.generate(prompt, max_new_tokens=256)\n",
        "    return answer, docs\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def qa_interface(file_path, query):\n",
        "    # file_path is str from Gradio File when type=\"filepath\"\n",
        "    answer, source_docs = rag_answer(file_path, query)\n",
        "    return answer\n",
        "\n",
        "rag_app = gr.Interface(\n",
        "    fn=qa_interface,\n",
        "    inputs=[\n",
        "        gr.File(label=\"Upload PDF File\", file_count=\"single\", file_types=['.pdf'], type=\"filepath\"),\n",
        "        gr.Textbox(label=\"Input Query\", lines=2, placeholder=\"Type your question here ...\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        "    title=\"PDF RAG QA Bot (Local Transformers)\",\n",
        "    description=\"Upload a PDF and ask a question. The bot uses local embeddings + Chroma + FLAN-T5.\"\n",
        ")\n",
        "\n",
        "# Launch\n",
        "rag_app.launch(server_name=\"0.0.0.0\", server_port=7860, share=True)\n"
      ],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}